# AI Agent Task: Automated CrewAI Python Script Generation

**Your Mission:** As an AI agent, your task is to generate a robust, production-quality CrewAI Python script based on the user's 'Initial Instruction Input' (provided within `@@@` delimiters). Adhere strictly to this protocol.

**Core Objective:** To produce a fully operational CrewAI Python application by interpreting the 'Initial Instruction Input'. This is achieved through three sequential internal phases:
1.  **Phase 1: Define Blueprint** - Develops a detailed Blueprint from the 'Initial Instruction Input'.
2.  **Phase 2: Design Crew Architecture Plan** - Creates a comprehensive CrewAI plan based *solely* on the Blueprint from Phase 1.
3.  **Phase 3: Construct Python Script** - Generates the final Python script *solely* from the Design-Crew-Architecture-Plan from Phase 2.

## Phase 1: Define Blueprint

**Input:** User-provided 'Initial Instruction Input' (from the `@@@` delimiters).

**Process:**
1.  Thoroughly analyze the 'Initial Instruction Input'.
2.  Identify ambiguities, missing information, or implicit requirements.
3.  Formulate the 'Blueprint' by augmenting and structuring the user's request, ensuring all pertinent details from the 'Initial Instruction Input' are captured.

**Output:** A 'Blueprint' document. This document is a structured representation generated by the AI, containing all sections defined below. It serves as the **direct and complete input** for Phase 2.

**'Blueprint' - Mandatory Sections:**

**A. Primary Objective:**
*   Content: Single, clear statement of the main goal.

**B. Deliverable(s):**
*   Content: Precise specification of ALL final outputs.
    *   Filename(s) and format(s) (e.g., `analysis_report.md`, `results.json`).
    *   Detailed structure (e.g., Markdown sections, JSON schema, data fields).
*   Constraint: This deliverable **MUST** be the direct result of a defined task or step in the execution outline.
*   Deliverable Success Criteria: How to assess the correctness and quality of the final deliverable(s).

**C. Information Assets & Technical Specifications:**
*   Content:
    *   List ALL required data sources/information types.
    *   **File types and their intended processing** (e.g., 'extract all text from PDF `main.pdf`', 'summarize DOCX content from `report.docx`', 'query data from `data.csv`'). This information is crucial for Phase 2 to select or design appropriate tools.
    *   Specific URLs for web tools or external services (if any).
    *   Descriptions of local files/databases (assume accessibility if mentioned).
    *   Keywords/strategies for research or data gathering (which may imply tool needs like web search).
    *   APIs (if any), including authentication details if provided in the input.
    *   List ALL technical specifications provided in the 'Initial Instruction Input' (e.g., required libraries, algorithms, performance constraints, specific tool versions, hardware/software environment constraints).
*   A list of requirements omitted from the final Blueprint, and the reasoning.

**D. Success Criteria (Overall Task):**
*   Content: Broader key questions, analyses, or content requirements the system must meet beyond the deliverable format.

**E. Operational Context:**
*   Content: Purpose of the output and its intended audience.

**F. Clarified Assumptions:**
*   Content: Explicitly state ALL assumptions made during Blueprint formulation (e.g., about data availability, tool compatibility, interpretation of ambiguous requests).

**G. Execution Outline:**
*   Content: A high-level, step-by-step workflow designed to achieve the Primary Objective.
    *   Each step should clearly define its purpose, **expected input (including type/format, e.g., 'text string', 'PDF file path')**, the operation to be performed, and **expected output (including type/format, e.g., 'markdown text', 'JSON data')**. This detail is vital for Phase 2 tool selection.
    *   This outline **MUST** incorporate any technical specifications identified in Section C.
    *   For each technical specification, provide clear reasoning for its integration into specific step(s), explaining its contribution or constraints.
    *   Detail the logical flow of data and operations between steps.

## Phase 2: Design Crew Architecture Plan

**Input:** The **complete 'Blueprint' document** generated in Phase 1. No other information source should be used.

**Process:** Based *solely* on the **complete and detailed 'Blueprint'**, design the optimal CrewAI configuration. This involves developing specifications for tasks, agents, and tools. Define all tool configurations and custom tools centrally, then reference them in tasks. Ensure all aspects of the Blueprint are addressed.

**Output:** A 'Design-Crew-Architecture-Plan' document. This document is a structured representation generated by the AI, containing all sections defined below. It serves as the **direct and complete input** for Phase 3. It must contain all specifications and configurations required for Phase 3 to construct the Python script without referring back to Phase 1.

**'Design-Crew-Architecture-Plan' - Mandatory Sections:**

**A. Workflow Process:**
*   Selected Process: `Process.sequential` OR `Process.hierarchical`.
*   Justification: Explain WHY this process is optimal for the 'Blueprint's Execution Outline'.
*   Manager LLM Specification (If `Process.hierarchical`):
    *   `model`: Selected model name (e.g., 'gemini/gemini-2.5-pro-preview-05-06'). MUST be from Approved LLM List (see Phase 3).
    *   `temperature`: **MUST BE 0.0**.
    *   `api_key`: (Optional) Environment variable name for the API key.
    *   `config_params`: (Optional) Specific configuration dictionary for the LLM.
    *   `multimodal`: (Optional, default `False`) `True` if the manager LLM needs multimodal capabilities.
    *   Rationale: Justify the choice of this manager LLM and its `multimodal` setting (if `True`).

**B. Agent Cadre:** For EACH agent, define:
*   Role: Concise functional title.
*   Goal: Specific objective aligned with the 'Blueprint'.
*   Backstory: Narrative reinforcing expertise for the Goal.
*   LLM Specification:
    *   `model`: Selected model name (e.g., 'gemini/gemini-2.5-flash-preview-05-20'). MUST be from the 'Approved LLM List' (see Phase 3).
    *   `temperature`: **MUST BE 0.0**.
    *   `api_key`: (Optional) Environment variable name for the API key.
    *   `config_params`: (Optional) Any specific configuration dictionary required for the LLM.
    *   `multimodal`: (Optional, default `False`) `True` if this agent's LLM needs multimodal capabilities.
*   LLM Rationale: Justify the choice of THIS specific model for THIS agent's Role and Goal, referencing its capabilities (e.g., reasoning, generation, speed, multimodal support) relevant to the 'Blueprint'.
*   Tools (Agent's Toolkit): List general tool *types/capabilities* (e.g., Web Search, File Writing, PDF Text Extraction) this agent is expected to leverage. The specific tool *instances* an agent uses will be derived from the 'Enabling Tool(s)' of the tasks assigned to it.
*   Tool Rationale (Agent Level): Explain WHY this overall toolkit (as a collection of capabilities) is essential for this agent.
*   `allow_delegation`: `True` or `False`.
*   Delegation Rationale: Justify the choice for `allow_delegation`.

**C. Tool Configuration Repository:** (Defines all unique tool configurations)
*   For EACH unique tool configuration:
    *   `config_id`: A unique identifier/variable name for this tool configuration (e.g., 'serper_search_main', 'report_writer_tool', 'pdf_text_extractor').
    *   `tool_type`: The CrewAI tool class (e.g., 'SerperDevTool', 'FileWriterTool') or the `class_name` of a custom tool (defined in Section D).
    *   `is_custom_tool`: `True` if `tool_type` refers to a class name from Section D, `False` otherwise.
    *   `tool_selection_justification`: Based on the 'Blueprint's' information assets (especially file types and processing needs like "extract all text from PDF") and execution outline, justify why this specific `tool_type` (and its configuration) is chosen.
        *   If it's a standard CrewAI tool (e.g., `SerperDevTool`, `FileWriterTool`, `WebsiteSearchTool`, `PDFSearchTool`), explain its suitability for the required input/output.
        *   If a custom tool is being proposed (`is_custom_tool: True`), this justification must clearly state why no standard CrewAI tool (even with varied configurations) can meet the specific requirements (e.g., `FileReadTool` for non-plain-text files like PDFs, `PDFSearchTool` being for Q&A not full text extraction when "extract all text" is needed).
    *   `initialization_params`: A dictionary of parameters for initializing the tool instance in Phase 3.
        *   For API keys, specify the environment variable name (e.g., `{'api_key': 'SERPER_API_KEY'}`).
        *   **RAG Tool Configuration (If this `tool_type` is a RAG tool like `WebsiteSearchTool`, `PDFSearchTool`, `CSVSearchTool`, or a custom RAG tool):** This `initialization_params` dictionary MUST include the `config` dictionary for LLM/embedder.
            ```python
            # Example for initialization_params of a RAG tool:
            # {
            #   "source_identifier": "https://example.com", # Or file_path, etc.
            #   "config": { # RAG config dictionary
            #     "llm": {
            #         "provider": "google", # Or other provider
            #         "config": {
            #             "model": "gemini/gemini-2.5-flash-preview-05-20", # Must be from approved list or justified
            #             "temperature": 0.0,
            #             # "multimodal": False, # If applicable
            #             # "api_key": "GOOGLE_API_KEY" # If needed
            #         },
            #     },
            #     "embedder": {
            #         "provider": "ollama", # Or other provider
            #         "config": {
            #             "model": "nomic-embed-text:latest",
            #             # "url": "http://localhost:11434/api/" # If needed
            #         },
            #     },
            #   }
            # }
            ```
            *   Rationale for RAG: Provide rationale for LLM (temp=0.0, ideally approved list or justified if specialized, consider multimodal) and embedder model choices within this RAG config.
    *   `input_data_requirements`: General description of the expected input data format, type, and source for this tool configuration (e.g., 'text string', 'local PDF file path given by user', 'URL').
    *   `output_data_requirements`: General description of the expected output data format and type from this tool configuration (e.g., 'markdown string', 'JSON object', 'file path of written content').

**D. Custom Tool Definitions:** (Define ONLY if `tool_selection_justification` in Section C determines a custom tool is necessary)
*   Condition: A custom tool is proposed ONLY if no built-in `crewai_tool` (analyzed first via Section C) can perform a critical 'Blueprint' function (e.g., "extract all text from a PDF" might require a custom tool if `PDFSearchTool` is unsuitable for that specific goal).
*   For EACH custom tool:
    *   `class_name`: Python class name (e.g., 'PDFTextExtractorTool'). This `class_name` is used as `tool_type` in Section C.
    *   `tool_name_attr`: The 'name' attribute for the tool (e.g., 'PDF Full Text Extractor').
    *   `description_attr`: The 'description' attribute for the tool.
    *   `args_schema_class_name`: (Optional) Pydantic model class name (defined in Section F) for input arguments.
    *   `_run_method_signature`: Define the signature, e.g., `def _run(self, file_path: str) -> str:`.
    *   `_run_method_logic_description`: Detailed textual description of the `_run` method, including libraries to use (e.g., `PyMuPDF` for PDF text extraction), and error handling.
    *   `justification_for_custom_tool`: Explicitly state why this custom tool is essential, referencing specific requirements from the 'Blueprint' (e.g., "Blueprint requires extracting all raw text from `main.pdf`. Standard `FileReadTool` cannot process PDFs. `PDFSearchTool` is for Q&A, not full text extraction. A custom tool using `PyMuPDF` is needed to meet this specific 'extract all text' requirement."). This information MUST be passed to Phase 3 for inclusion as comments.

**E. Task Roster:** Detail EACH task. At least ONE task **MUST** produce the final 'Deliverable(s)'.
*   Task Identifier: Unique name (e.g., `pdf_extraction_task`, `analysis_task`).
*   Description: Detailed operational prompt for the agent, derived from 'Blueprint's Execution Outline'.
*   Assigned Agent Role: The `Role` of the designated agent (from Section B).
*   Expected Output: Clear definition of what this task produces (e.g., "A string containing all extracted text from the PDF.", "A markdown report with sections X, Y, Z.").
    *   If this task produces one of the 'Blueprint's Deliverable(s)', specify the filename and format here.
    *   If structured output is required and validated, reference an `output_pydantic_model` (see below).
*   Enabling Tool(s):
    *   List one or more `config_id`(s) from Section C that this task will use.
    *   For each `config_id`, optionally specify:
        *   `task_specific_expected_input`: (Optional) Clarification if the tool's input for *this specific task* needs further detail beyond `input_data_requirements` in Section C.
        *   `task_specific_expected_output`: (Optional) Clarification if the tool's output for *this specific task* needs further detail beyond `output_data_requirements` in Section C.
*   Tool Rationale (Task Level): Justify WHY these specific tool configurations (referenced by `config_id`) are most effective for THIS task. This rationale **MUST** compare the task's input/output needs (derived from Blueprint's Execution Outline) with the `input_data_requirements` and `output_data_requirements` of the chosen tool(s) from Section C. Reiterate why standard tools are chosen or why a custom tool (via its `config_id`) is necessary for this task's specific function.
*   Context: List prerequisite task *identifiers*, if any.
*   `output_pydantic_model`: (Optional) `class_name` of a Pydantic model from Section F if this task's output should be an instance of that model.
*   `output_pydantic_model_rationale`: (If `output_pydantic_model` is used) Justify why Pydantic validation/structure is beneficial for this specific task's output (e.g., "Ensures the analysis result is always in the correct structured format for downstream tasks or final deliverable.").

**F. Structured Data Handling (Pydantic):**
*   Usage: "Yes" or "No".
*   Rationale (If "Yes"): Explain HOW Pydantic models will enhance data integrity, inter-task communication (by providing clear data contracts for task outputs via `output_pydantic_model` in Section E), or structure final deliverables for THIS 'Blueprint'.
*   Model Definitions (If "Yes"): For each Pydantic model:
    *   `class_name`: Python class name (e.g., `AnalysisResultModel`).
    *   `fields`: A dictionary of field names to their Python types (e.g., `{'report_title': 'str', 'score': 'float', 'findings': 'List[str]'}`).

**G. Crew Memory:**
*   Activation: "Yes" or "No" for `memory=True` in the Crew object.
*   Rationale (If "Yes"): Explain WHY memory is crucial for THIS 'Blueprint's Primary Objective'.

**H. Memory Embedder (If Crew Memory is Active):**
*   Condition: If `memory=True`.
*   Configuration: Specify the dictionary structure for the `embedder` parameter in the Crew object.
    ```python
    # embedder_config_for_crew = {
    #     "provider": "ollama", # Or other provider
    #     "config": {
    #         "model": "nomic-embed-text:latest", # Specify model
    #         # "url": "http://localhost:11434/api/", # If applicable
    #     }
    # }
    ```
*   Rationale: Provide rationale for embedder model choice and provider.

**I. Internal Review Checklist (Perform this self-correction):**
*   **Blueprint Adherence:** Does this Plan fully address ALL 'Blueprint' requirements (Primary Objective, Deliverables, Information Assets including file processing needs, Execution Outline details)?
*   **Tooling Strategy:**
    *   Centralization: Are ALL tool configurations uniquely defined in 'Tool Configuration Repository' (Section C)? Are ALL custom tool structures defined in 'Custom Tool Definitions' (Section D)?
    *   Justification: Is `tool_selection_justification` in Section C robust for each tool, clearly explaining standard vs. custom choices based on Blueprint's input/output/file processing needs?
    *   Task Assignment: Do tasks in 'Task Roster' (Section E) correctly reference `config_id`s? Is 'Tool Rationale (Task Level)' clear, justifying tool choices against task requirements and the tool's specified `input_data_requirements`/`output_data_requirements`?
    *   Custom Tool Necessity: If custom tools are used, is `justification_for_custom_tool` in Section D compelling, confirming no standard tool (from `crewai_tools` or other configurations in Section C) is adequate for the specific function described in the Blueprint (e.g., "extract all text from PDF")?
*   **Output Path & Format:** Is the path to 'Blueprint's Deliverable(s)' via a specific task's output clear? Are output formats (including Pydantic models via `output_pydantic_model`) for tasks clearly defined where necessary?
*   **LLM Compliance:** Are ALL LLMs (agents, manager, RAG LLMs) specified with `temperature=0.0`? Are agent/manager LLMs from the approved list? Is `multimodal` considered and justified if `True`?
*   **Pydantic Usage:** If Pydantic models are used (Section F "Yes"), is their application justified and correctly linked to task outputs (`output_pydantic_model` in Section E)?
*   **Data Flow & Context:** Is inter-task data flow clear? Are task contexts correctly defined?
*   **Completeness for Phase 3:** Does this plan contain ALL information (LLM configs including `multimodal`, tool configs including RAG and input/output requirements, agent/task details, custom tool logic descriptions and justifications, Pydantic models) needed for Phase 3 to write the script without referring to Phase 1?
*   **Efficiency:** Is the design appropriately complex? Are tools efficiently reused via the central repository?

## Phase 3: Construct Python Script

**Input:** The **complete 'Design-Crew-Architecture-Plan' document** generated in Phase 2. No other information source should be used.

**Process:** Construct the Python script by meticulously implementing all specifications, configurations, and logic detailed in the **entirety of the 'Design-Crew-Architecture-Plan' input document**.

**Output:** The final, runnable CrewAI Python script.

**Script Structure & Content Requirements:**

**Environment Setup (Order is CRITICAL):**
```python
import os
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv()) # MUST BE CALLED EARLY
```

**Core Imports:**
```python
from crewai import LLM, Agent, Task, Crew, Process
# Import ALL specific standard tools used (e.g., from crewai_tools) based on 'tool_type' in Section C of the Plan
# Example: from crewai_tools import SerperDevTool, WebsiteSearchTool, FileWriterTool, FileReadTool, PDFSearchTool
# Import BaseTool if any custom tools are defined in Section D of the Plan
# from crewai.tools import BaseTool
# Import Pydantic BaseModel if any Pydantic models are defined in Section F of the Plan
# from pydantic import BaseModel
```
*(The AI generating the script **MUST** uncomment `BaseTool`, `BaseModel`, and specific tool imports only if they are needed based on the Design-Crew-Architecture-Plan).*

**API Key Access:**
*   Method: Use `os.getenv("YOUR_API_KEY_NAME")` for ALL API keys, where "YOUR_API_KEY_NAME" is derived from `api_key` fields in the 'Design-Crew-Architecture-Plan'.
*   Constraint: **NO HARDCODED SECRETS.**

**LLM Instantiation:**
*   Instantiate LLM objects as defined in the 'Design-Crew-Architecture-Plan' (Section B: Agent Cadre - LLM Specification, and Section A for `manager_llm` if applicable).
*   Use the `model`, `temperature` (must be 0.0), `api_key` (obtained via `os.getenv` using the specified `api_key`), `config_params`, and `multimodal` settings from the Plan.
*   Constraint: `temperature=0.0` for ALL instances.
*   Constraint: Use ONLY approved LLMs for Agent/Manager roles.
*   **Approved LLM List:**
    *   `gemini/gemini-2.5-flash-preview-05-20`
    *   `gemini/gemini-2.5-pro-preview-05-06`
    *   `deepseek/deepseek-chat`
*   Example:
    ```python
    # llm_gemini_flash = LLM(
    #     model="gemini/gemini-2.5-flash-preview-05-20", # From Plan
    #     temperature=0.0,
    #     api_key=os.getenv("GOOGLE_API_KEY"), # "GOOGLE_API_KEY" from Plan's api_key
    #     # multimodal=True # If True in Plan's LLM Specification
    #     # config_params={'base_url': '...'} # If in Plan's LLM Specification
    # )
    ```

**Custom Tool & Pydantic Model Definitions (If applicable):**
*   If 'Custom Tool Definitions' (Section D) or 'Structured Data Handling' (Section F with "Yes" for Usage) are present in the 'Design-Crew-Architecture-Plan':
    *   Implement Python class definitions for ALL custom tools from Section D, using their `class_name`, `tool_name_attr`, `description_attr`, `args_schema_class_name` (if any), `_run_method_signature`, and `_run_method_logic_description`.
        *   **CRITICAL**: Include the `justification_for_custom_tool` (from Section D of the Plan) as a prominent comment within or above each custom tool class definition.
        *   Implement basic `try-except` blocks within custom tool `_run` methods for robustness, based on `_run_method_logic_description`.
    *   Implement Pydantic model class definitions specified in Section F of the Plan.

**Tool Instantiation:**
*   Instantiate ALL tools based on the 'Tool Configuration Repository' (Section C) from the 'Design-Crew-Architecture-Plan'.
*   For each entry in Section C:
    *   Use the `config_id` as the Python variable name for the tool instance.
    *   Use `tool_type` to determine the class to instantiate (e.g., `SerperDevTool`, or a custom tool `class_name` from Section D).
    *   Use `initialization_params` for instantiation arguments. This includes any RAG `config` dictionaries.
    *   Ensure API keys within `initialization_params` are fetched using `os.getenv()`.
    *   Ensure LLMs within RAG `config` also have `temperature=0.0` and use `multimodal` settings if specified in the Plan for that RAG LLM.

**Agent Definitions:**
*   Define agents as per 'Agent Cadre' (Section B) in the 'Design-Crew-Architecture-Plan'.
*   Instantiate agents using their `Role`, `Goal`, `Backstory`.
*   Assign the correct LLM instance (instantiated earlier) based on the `LLM Specification` for that agent.
*   The `tools` list for an agent **MUST** be a list of tool *instances* (instantiated in the previous step). To compile this list:
    1.  Identify all tasks assigned to this agent from the 'Task Roster' (Section E of the Plan).
    2.  For these tasks, collect all unique `config_id`s listed in their 'Enabling Tool(s)'.
    3.  Map these `config_id`s to the corresponding tool instances created during 'Tool Instantiation'.
*   `verbose=True` is **MANDATORY**.
*   Set `allow_delegation` based on the value specified in the Plan for that agent.

**Task Definitions:**
*   Define tasks as per 'Task Roster' (Section E) in the 'Design-Crew-Architecture-Plan', using their `Task Identifier` as the Python variable name.
*   Use the task `Description`.
*   Assign the agent instance that matches the `Assigned Agent Role`.
*   The `tools` parameter for each Task **MUST** be a list of specific tool INSTANCES (instantiated earlier). These instances correspond to the `config_id`(s) listed in that task's 'Enabling Tool(s)' in the Plan.
*   If `Context` is specified, ensure the task objects (Python variables) are correctly passed.
*   If the task's `Expected Output` in the Plan specifies a filename (e.g., for a deliverable), ensure the task is configured to produce this (e.g., by ensuring one of its tools, like a `FileWriterTool` referenced by its `config_id`, is initialized with or receives the filename).
*   If `output_pydantic_model` is specified in the Plan for this task (Section E), set the `output_pydantic` parameter of the Task to the corresponding Pydantic class (defined earlier).

**Crew Assembly:**
*   Create the Crew instance.
*   `agents=[...]`: List of instantiated agent objects.
*   `tasks=[...]`: List of instantiated task objects.
*   `process`: Set to `Process.sequential` or `Process.hierarchical` as defined in the 'Design-Crew-Architecture-Plan' (Section A).
*   `memory`: Set to `True` or `False` based on 'Crew Memory' activation (Section G of the Plan).
*   `embedder`: If memory is active, use the `embedder` configuration dictionary specified in Section H of the Plan.
*   `manager_llm`: If `Process.hierarchical`, assign the instantiated manager LLM object, as specified in Section A of the Plan.
*   `verbose=True` is **MANDATORY**.

**Execution Block:**
```python
if __name__ == "__main__":
    print("## Initializing Crew...")
    # inputs_dict = {"example_key": "example_value"} # Define if kickoff requires inputs, based on Blueprint/Plan
    # results = your_crew_instance.kickoff(inputs=inputs_dict)
    results = your_crew_instance.kickoff() # Use if no dynamic inputs specified in Plan
    print("\n## Crew Operation Complete.")
    print("Final Results:")
    print(results) # This will print the raw output of the last task, or structured data if Pydantic was used.

    # Example for checking a file deliverable (adapt based on Plan's deliverable spec):
    # final_deliverable_filename = "output_report.md" # Get this from the Plan if specified
    # if final_deliverable_filename and os.path.exists(final_deliverable_filename):
    #    print(f"\nDeliverable '{final_deliverable_filename}' generated at: {os.path.abspath(final_deliverable_filename)}")
    #    # Optionally, print file content if it's text-based and small
    #    # with open(final_deliverable_filename, 'r') as f:
    #    #     print("\n--- File Content ---")
    #    #     print(f.read())
    #    #     print("--- End of File Content ---")
    # elif final_deliverable_filename:
    #    print(f"\nDeliverable file '{final_deliverable_filename}' was expected but not found. Review task outputs in 'results'.")

```

**Internal Review Checklist (Phase 3 Self-Correction - Perform before finalizing script):**
*   **Plan Adherence:** Does the script accurately implement ALL specifications from the 'Design-Crew-Architecture-Plan' (LLMs, tools, agents, tasks, process, memory, Pydantic models, custom tool logic, RAG configs)?
*   **Python Syntax & Imports:** Is the script syntactically correct Python 3? Are all necessary modules imported (e.g., `BaseTool`, `BaseModel` *only if used*, specific `crewai_tools` as per Plan)? Run a linter or syntax check if possible.
*   **Environment & API Keys:** Is `load_dotenv(find_dotenv())` called at the very beginning? Are all API keys accessed via `os.getenv()` using correct environment variable names from the Plan? **VERIFY NO HARDCODED SECRETS.**
*   **LLM Configuration:** Are all LLM instances (agents, manager, RAG LLMs) configured with `temperature=0.0` and correct models, API keys, `config_params`, and `multimodal` settings as per the Plan?
*   **Tool Instantiation & Configuration:** Are all tools instantiated correctly with parameters from the 'Tool Configuration Repository' (Section C of Plan), including RAG `config` dictionaries? Are tool instances correctly assigned to Python variables matching `config_id`?
*   **Custom Tools & Pydantic:** If defined in the Plan, are custom tool classes implemented with correct signatures, logic (from `_run_method_logic_description`), `args_schema`? Is the `justification_for_custom_tool` included as a comment? Are Pydantic models defined correctly?
*   **Agent Configuration:** Are agents defined with correct roles, goals, backstories, LLMs, `verbose=True`, `allow_delegation`? Is their `tools` list correctly compiled from assigned tasks' `config_id`s (mapping to tool instances)?
*   **Task Configuration:** Are tasks defined with correct descriptions, agents, and contexts? Is the `tools` list for each task correctly populated with tool *instances*? Is `output_pydantic` set to the Pydantic class if specified in the Plan?
*   **Crew Assembly:** Is the Crew object assembled with the correct agents, tasks, process, memory settings (including embedder if applicable), and manager LLM (if hierarchical)? Is `verbose=True` set for the Crew?
*   **Deliverable Path:** If a task is meant to produce a file deliverable (as per Blueprint and Plan), is it configured to do so (e.g., `FileWriterTool` with correct path/filename)? Does the execution block attempt to report the deliverable's status/location?
*   **Execution Logic:** Does the `kickoff()` method run correctly? Are inputs (if any) handled as per the Plan? Is the `results` variable captured and printed?
*   **Readability & Comments:** Is the code well-formatted? Are custom logic sections (especially in tools) adequately commented, including the justification for custom tools?


@@@

@@@
